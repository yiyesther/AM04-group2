---
title: "Data Science I, Workshop I: Predicting interest rates at the Lending Club"
author: "Change this to your GROUP NUMBER"
date: "Date of submission on Canvas"
output:
  html_document:
    theme: cerulean
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, load_libraries, include = FALSE, message=FALSE}
library(tidyverse) # the usual stuff: dplyr, readr, and other goodies
library(lubridate) # to handle dates
library(GGally) # for correlation-scatterplot matrix
library(car) # vif() function to check for multicolinearity
library(ggfortify) # to produce residual diagnostic plots
library(rsample) # to split dataframe in training- & testing sets
library(here) # to read files and organise data
library(janitor) # clean_names()
library(broom) # use broom:augment() to get tidy table with regression output, residuals, etc
library(huxtable) # to get summary table of all models produced
library(caret) # to train more advanced models (k-fold cross-validation, stepwise regression, LASSO)
library(zoo) #to allow for time series operations
library(skimr)
```


This workshop partially replicates the analysis presented in class (lectures 1 and 2) and builds on it. You are to work in your study group. Feel free to refer to the "Lending_Club_Session1_and_2.html" if you get stuck.

The workshop consists of 11 questions. Submit one report (one file) per study group. 

*Please write your answer below each question and submit a knitted RMD markup file as the HTML file via Canvas, within 6 days after the end of this workshop. Please note that there is a 20% penalty on your Workshop Report 1 grade if you submit an RMD file.* 

Please keep your answers concise -- focus on answering what you are asked and use your data science work to justify your answers. Do not focus on the process you have followed to reach the answer.

# Load and prepare the data

We start by loading the data to R in a dataframe.

```{r, load_data, warning=FALSE, message=FALSE}

lc_raw <- read_csv("LendingClub Data.csv",  skip=1) %>%  #since the first row is a title we want to skip it. 
  clean_names() # use janitor::clean_names()
```

# ICE the data: Inspect, Clean, Explore

Any data science engagement starts with ICE. Inspect, Clean and Explore the data. For this workshop I have cleaned the data for you. 

```{r, ICE, warning=FALSE, message=FALSE}
glimpse(lc_raw) 

lc_clean<- lc_raw %>%
  dplyr::select(-x20:-x80) %>% #delete empty columns
  filter(!is.na(int_rate)) %>%   #delete empty rows
  mutate(
    issue_d = mdy(issue_d),  # lubridate::mdy() to fix date format
    term = factor(term_months),     # turn 'term' into a categorical variable
    delinq_2yrs = factor(delinq_2yrs) # turn 'delinq_2yrs' into a categorical variable
  ) %>% 
  dplyr::select(-emp_title,-installment, -term_months, everything()) #move some not-so-important variables to the end. 


glimpse(lc_clean) 

    
```

The data is now in a clean format stored in the dataframe "lc_clean." 

# Q1. Explore the data by building some visualizations as suggested below. Please add at least TWO visualizations of your own. 


```{r, skim_data, warning=FALSE, message=FALSE}
skim(lc_clean)
```

Provide your answers in the code block below. (Look at "Lending_Club_Session1_and_2.html" for some hints on how to do this using ggplot.)

```{r, data_visualisation, warning=FALSE, message=FALSE}
# Build a histogram of interest rates. Make sure it looks nice!

ggplot(lc_clean, aes(x=int_rate))+  
  geom_histogram(binwidth=0.01)+scale_x_continuous(labels = scales::percent) +labs(x="Interest Rate")
```


```{r,histogram_color,warning=FALSE, message=FALSE}
# Build a histogram of interest rates but use different color for loans of different grades 

ggplot(lc_clean, aes(x=int_rate, fill=grade))+  geom_histogram(binwidth=0.01)+scale_x_continuous(labels = scales::percent)+ labs(x="Interest Rate") 
``` 

```{r,scatter,warning=FALSE, message=FALSE}
# Produce a scatter plot of loan amount against interest rate and add visually the line of best fit
ggplot(lc_clean[seq(1, nrow(lc_clean), 10), ] , aes(y=loan_amnt, x=int_rate)) + 
  geom_point(size=0.1, alpha=0.5)+ 
  geom_smooth(method="lm", se=0) + labs(y="Loan Amount ($)", x="Interest Rate")
```



```{r,scatter_1,warning=FALSE, message=FALSE}
# Produce a scatter plot of annual income against interest rate and add visually the line of best fit 

ggplot(lc_clean[seq(1, nrow(lc_clean), 10), ] , aes(y=annual_inc, x=int_rate)) + 
  geom_point(size=0.1, alpha=0.5)+ 
  geom_smooth(method="lm", se=0) + 
  labs(y="Annual Income ($)", x="Interest Rate")
```


```{r,box,warning=FALSE, message=FALSE}
# In the same axes, produce box plots of the interest rate for every value of delinquencies

ggplot(lc_clean , aes(y=int_rate, x=delinq_2yrs, fill= delinq_2yrs)) + 
  geom_boxplot()+
  # geom_jitter()+
  theme_bw()+
   scale_y_continuous(labels=scales::percent)+
  theme(legend.position = "none")+
  labs(
    title = "Do delinquencies in the last two years impact interest rate charged?",
    x= "Number of delinquecies in last two years", y="Interest Rate"
  )

```


# Add 2 visualizations of your own

```{r,density,warning=FALSE, message=FALSE}
ggplot(lc_clean, aes(x=int_rate, fill=grade, alpha = 0.2))+  
  geom_density()+
  facet_grid(rows = vars(grade))+
  theme_bw()+
  theme(legend.position = "none")+
  scale_x_continuous(labels = scales::percent)+ labs(x="Interest Rate",y="Density")
```

```{r,box_1,warning=FALSE, message=FALSE}
ggplot(lc_clean, aes(x=home_ownership, y=int_rate, fill=home_ownership))+  
  geom_boxplot()+
  theme_bw()+
  theme(legend.position = "none")+
  coord_flip()+ scale_y_continuous(labels=scales::percent)+
  labs(y="Interest Rate", x="Home Ownership")+
  NULL
```
#comment

# Estimate simple linear regression models

We start with a simple but quite powerful model.

```{r, simple regression, message=FALSE,warning=FALSE}
# Use the lm command to estimate a regression model with the following variables "loan_amnt",  "term", "dti", "annual_inc", and "grade"

model1<-lm(formula = int_rate ~ loan_amnt + term + dti + annual_inc + grade, data = lc_clean)
summary(model1)

#comment 
```

## Q2. Answer the following questions about model 1 above.{-}

a. Are all variables statistically significant?
b. Interpret all the coefficients in the regression.
c. How much explanatory power does the model have? 
d. How wide would the 95% confidence interval of any prediction based on this model be? 

>Answer here:

# Feature Engineering

Let's build progressively more complex models, with more features.

```{r, Feature Engineering, message=FALSE, warning=FALSE}
#Add to model 1 an interaction between loan amount and grade. Use the "var1*var2" notation to define an interaction term in the linear regression model. This will add the interaction and the individual variables to the model. 

model2 <-lm(formula = int_rate ~ loan_amnt + term + dti + annual_inc + grade + loan_amnt*grade, data = lc_clean)

summary(model2)
```


```{r,model3,warning=FALSE, message=FALSE}
#Add to the model you just created above the square and the cube of annual income. Use the poly(var_name,3) command as a variable in the linear regression model.  

model3 <-lm(formula = int_rate ~ loan_amnt + term + dti + annual_inc + 
              grade + loan_amnt*grade +
              poly(annual_inc, 3),
            data = lc_clean)

summary(model3)
```
>comment on the model

```{r,model4,warning=FALSE, message=FALSE}
#Continuing with the previous model, instead of annual income as a continuous variable break it down into quartiles and use quartile dummy variables. You can do this with the following command. 
  
lc_clean <- lc_clean %>% 
  mutate(quartiles_annual_inc = as.factor(ntile(annual_inc, 4)))

model4 <- lm(formula = int_rate ~ loan_amnt + term + dti + quartiles_annual_inc + 
              grade + loan_amnt*grade+
              poly(annual_inc, 3),
            data = lc_clean)
summary(model4) 
```

```{r,anova,warning=FALSE, message=FALSE}
#Compare the performance of these four models using the anova command
anova(model1, model2, model3, model4)
```

```{r}
#Presentation of all results
huxtable::huxreg(model1, model2, model3, model4, 
                 number_format = "%.3f")
```


## Q3. Answer the following questions {-}
a. Which of the four models has the most explanatory power in sample?
b. In model 2, how do you interpret the estimated coefficient of the interaction term between grade B and loan amount? 
c. The problem of multicollinearity describes situations in which one feature is correlated with other features (or with a linear combination of other features). If your goal is to use the model to make predictions, should you be concerned about multicollinearity? Why, or why not?

>Answer here:

a. Models 2, 3 and 4 all have an adjusted R^2 value of 0.9204, and therefore, all 3 of these models can explain 92% of the variability in the data. Therefore, in theory, all 3 of these models have the same explanatory power and the further changes introduced in model 3 and 4 do not actually improve the explanatory power of the regression model. Models 2, 3 and 4 have an R^2 value that is approximately 0.076% higher than model 1. Therefore the explanatory power of all models are fairly similar. The p-value for all of these models is also < 2.2e-16. This means that for models 2, 3 and 4, the probability that the model has no explanatory power is less than 2.2e-16. 

b. In model 2, the coefficient of the interaction term between grade B and loan amount is -6.617e-08. The t-value for this term is -2.710 and the p-value is 0.00673. Based on this, although there are some interaction terms that are more significant, this coefficient is still considered to be significant. This is because it has a p-value < 0.05 - therefore we can deduce that this particular interaction term/parameter is significant at the 95% confidence level. 

c. Although multi-collinearity will make it harder to interpret coefficients in the model and it will affect the p-values associated with the coefficients. However, it will NOT affect the capabilities of the model to make predictions. In order to make accurate predictions, the behavior of every variable does not need to be explained, and therefore multi-collinearity will not be relevant to the model's predictive power. Multi-collinearity will however affect the variance associated with prediction. Therefore, although it should not affect the prediction, multi-collinearity should be kept low in order to minimise prediction variance and allow for better estimates of coefficients. 

# Out of sample testing
Let's check the predictive accuracy of model2 by holding out a subset of the data to use as a testing data set. This method is sometimes referred to as the hold-out method for out-of-sample testing. 

# Comment and explain each row of the code in the chunk below.

```{r, out of sample testing,warning=FALSE, message=FALSE}
# split the data in dataframe called "testing" and another one called  "training". The "training" dataframe should have 80% of the data and the "testing" dataframe 20%.
set.seed(4444)
train_test_split <- initial_split(lc_clean, prop = 0.80)
training <- training(train_test_split)
testing <- testing(train_test_split)



# Fit model2 on the training set 
model2_training<-lm(int_rate ~ loan_amnt + term+ dti + annual_inc + grade +grade:loan_amnt, training)

# Calculate the RMSE of the model in the training set (in sample)
rmse_training<-sqrt(mean((residuals(model2_training))^2))

# Use the model to make predictions out of sample in the testing set
pred<-predict(model2_training,testing)

# Calculate the RMSE of the model in the testing set (out of sample)
rmse_testing<- RMSE(pred,testing$int_rate)


```

## Q4. How much does the predictive accuracy of Model 2 deteriorate when we move from in sample to out of sample testing? Is this sensitive to the random seed chosen? Is there any evidence of overfitting? {-}

>Answer here:

Based on the code above, we can see that the RMSE 'in sample' (in the training set) is 0.01051. On the other hand, the RMSE 'out of sample' (in the testing set) is 0.01054. Therefore, there is an increase in the error is: 

```{r}
Percentage_Error_Increase <- 100* ((rmse_testing - rmse_training)/rmse_training)
Percentage_Error_Increase
```

This difference is very small, and therefore we can conclude that over-fitting will not be a problem. This difference also highlights the fact that the predictive accuracy of the model will decrease by 0.3213%. Changing the random seed will affect our results, however the result is not overly sensitive to the random seed. We use set.seed(4444) so that we can re-replicate the random seed again when re-running the code. 


# k-fold cross validation

We can also do out of sample testing using the method of k-fold cross validation. Using the caret package this is easy.

```{r, k-fold cross validation,warning=FALSE, message=FALSE}
#the method "cv" stands for cross validation. We re going to create 10 folds.  

control <- trainControl (
    method="cv",
    number=10,
    verboseIter=TRUE) #by setting this to true the model will report its progress after each estimation

#we are going to train the model and report the results using k-fold cross validation

plsFit<-train(
    int_rate ~ loan_amnt + term+ dti + annual_inc + grade +grade:loan_amnt ,
    lc_clean,
   method = "lm",
    trControl = control
   )
  

summary(plsFit)

```

## Q5. Compare the out-of-sample RMSE of 10-fold cross validation and the hold-out method. Are they different? Which do you think is more reliable? Are there any drawbacks to the k-fold cross validation method compared to the hold-out method? Determine the out-of-sample RMSE based on 5-fold or 15-fold cross validation.  


```{r}
rse = 0.01052
out_of_sample_rmse = sqrt((rse^2*37852)/37869)
out_of_sample_rmse
```

>Answer here:

The hold-out method creates certain issues by splitting the data into a training and validation data set. This reduces the data used - essentially we don't use every data point for training and for testing - which can affect our results. Comparing the RMSE of this to the 10-fold cross validation method shows that the out of sample RMSE for the 10 fold cross validation method is 0.010518, which is smaller than the out of sample RMSE for the 'hold-out method' (0.01054). This shows that, by overcoming the problems associated with splitting the dataset for training/prediction, we obtain a lower RMSE. The minor reduction in the error is as follows: 

```{r}
Percentage_Error_Decrease <- 100 * ((rmse_testing - out_of_sample_rmse)/rmse_testing)
Percentage_Error_Decrease
```

Evidently, based on the reduction of the rmse, the k-fold cross-validaiton method results are more reliable. This is expected as this method overcomes the problem associated with not using every data point for training and testing. As a result there are no evident drawbacks to using the k-fold cross-validation method when compared to the hold-out method. 

#5-fold and 15-fold

The out of sample rmse for 5-fold and 15-fold cross validation are determined as follows: 

```{r, k-fold cross validation - 5}
#The method "cv" stands for cross validation. We re going to create 5 folds.  
control5 <- trainControl (
    method="cv",
    number=5,
    verboseIter=TRUE) #by setting this to true the model will report its progress after each estimation

#we are going to train the model and report the results using k-fold cross validation

plsFit5<-train(
    int_rate ~ loan_amnt + term+ dti + annual_inc + grade +grade:loan_amnt ,
    lc_clean,
   method = "lm",
    trControl = control5
   )
summary(plsFit5)
```

```{r, k-fold cross validation - 15}
#The method "cv" stands for cross validation. We re going to create 15 folds.  
control15 <- trainControl (
    method="cv",
    number=15,
    verboseIter=TRUE) #by setting this to true the model will report its progress after each estimation

#we are going to train the model and report the results using k-fold cross validation

plsFit15<-train(
    int_rate ~ loan_amnt + term+ dti + annual_inc + grade +grade:loan_amnt ,
    lc_clean,
   method = "lm",
    trControl = control15
   )
summary(plsFit15)
```

For the 5 and 15 fold cross validation, the rse will be the same, and therefore the rmse will also be the same. The number of folds clearly do not affect the model estimates, because we have a large data set with enough observations. The model is not overfitted. 


# Sample size estimation and learning curves

We can use the hold out method for out-of-sample testing to check if we have a sufficiently large sample to estimate the model reliably. The idea is to set aside some of the data as a testing set. From the remaining data draw progressively larger training sets and check how the performance of the model on the testing set changes. If the performance no longer improves with larger training sets we know we have a large enough sample.  The code below does this. Examine it and run it with different random seeds. 

```{r, learning curves,warning=FALSE, message=FALSE}
#select a testing dataset (25% of all data)
set.seed(12)

train_test_split <- initial_split(lc_clean, prop = 0.75) #comemnt on evey line
remaining <- training(train_test_split)
testing <- testing(train_test_split)

#We are now going to run 30 models starting from a tiny training set drawn from the training data and progressively increasing its size. The testing set remains the same in all iterations.

#initiating the model by setting some parameters to zero
rmse_sample <- 0
sample_size<-0
Rsq_sample<-0

for(i in 1:30) {
#from the remaining dataset select a smaller subset to training the data
set.seed(100)
sample

  learning_split <- initial_split(remaining, prop = i/200)
  training <- training(learning_split)
  sample_size[i]=nrow(training)
  
  #traing the model on the small dataset
  model3<-lm(int_rate ~ loan_amnt + term+ dti + annual_inc + grade + grade:loan_amnt, training)
  #test the performance of the model on the large testing dataset. This stays fixed for all iterations.
  pred<-predict(model3,testing)
  rmse_sample[i]<-RMSE(pred,testing$int_rate)
  Rsq_sample[i]<-R2(pred,testing$int_rate)
}
plot(sample_size,rmse_sample)
plot(sample_size,Rsq_sample)
```

## Q6. Using the learning curves above, approximately how large of a sample size would we need to estimate model 3 reliably? Once we reach this sample size, if we want to reduce the prediction error further what options do we have?{-}

>Answer here:

By running the learning curves code with different seeds, we can generally see that the rmse and the r^2 completely stabilise once the sample size is around 1000. The fact that the stabilisation occurs at around n=1000, shows us that the sample is sufficiently large - we have enough data and estimation error will not be a big issue. 

Once we reach this sample size, any further changes in the result of rsme and r^2 are likely due to overfitting problems. If we want to reduce the prediction error further, we could selectively remove outliers in the dataset. Additionally, we could introduce further explanatory variables (where appropriate) and also potentially incorporate (log) transformations for some variables, in order to produce a more representative model. 

# Regularization using LASSO regression

If we are in the region of the learning curve where we do not have enough data, one option is to use a regularization method such as LASSO.

Let's try to estimate a large and complicated model (many interactions and polynomials) on a small training dataset using OLS regression and hold-out validation method.

```{r, OLS model overfitting,warning=FALSE, message=FALSE}

#split the data in testing and training. The training test is really small.
set.seed(1234)
train_test_split <- initial_split(lc_clean, prop = 0.01)
training <- training(train_test_split)
testing <- testing(train_test_split)

model_lm<-lm(int_rate ~ poly(loan_amnt,3) + term+ dti + annual_inc + grade +grade:poly(loan_amnt,3):term +poly(loan_amnt,3):term +grade:term, training)
predictions <- predict(model_lm,testing)

# Model prediction performance
data.frame(
  RMSE = RMSE(predictions, testing$int_rate),
  Rsquare = R2(predictions, testing$int_rate)
)
```

Not surprisingly this model does not perform well -- as we knew form the learning curves we constructed for a simpler model we need a lot more data to estimate this model reliably. Try running it again with different seeds. The model's performance tends to be sensitive to the choice of the training set.

LASSO regression offers one solution -- it extends the OLS regression by penalizing the model for setting any coefficient estimate to a value that is different from zero. The penalty is proportional to a parameter $\lambda $ (pronounced lambda). This parameter cannot be estimated directly (and for this reason sometimes it is referred to as hyperparameter). $\lambda $  will be selected through k-fold cross validation so as to provide the best out-of-sample performance.  As a result of the LASSO procedure, only those features that are more strongly associated with the outcome will have non-zero coefficient estimates and the estimated model will be less sensitive to the training set. Sometimes LASSO regression is referred to as regularization. 

```{r, LASSO compared to OLS, warning=FALSE, message=FALSE}
# we will look for the optimal lambda in this sequence (we will try 1000 different lambdas, feel free to try more if necessary)
lambda_seq <- seq(0, 0.01, length = 1000)

# lasso regression using k-fold cross validation to select the best lambda

lasso <- train(
 int_rate ~ poly(loan_amnt,3) + term+ dti + annual_inc + grade +grade:poly(loan_amnt,3):term +poly(loan_amnt,3):term +grade:term,
 data = training,
 method = "glmnet",
  preProc = c("center", "scale"), #This option standardizes the data before running the LASSO regression
  trControl = control,
  tuneGrid = expand.grid(alpha = 1, lambda = lambda_seq) #alpha=1 specifies to run a LASSO regression. If alpha=0 the model would run ridge regression.
  )
```

```{r,summary_of_lasso,warning=FALSE, message=FALSE}
# Model coefficients
coef(lasso$finalModel, lasso$bestTune$lambda)
```

```{r,lambda,warning=FALSE, message=FALSE}
# Best lambda
lasso$bestTune$lambda

# Count of how many coefficients are greater than zero and how many are equal to zero

sum(coef(lasso$finalModel, lasso$bestTune$lambda)!=0)
sum(coef(lasso$finalModel, lasso$bestTune$lambda)==0)

# Make predictions
predictions <- predict(lasso,testing)

```

```{r,prediction_performance,warning=FALSE, message=FALSE}

# Model prediction performance

data.frame(
  RMSE = RMSE(predictions, testing$int_rate),
  Rsquare = R2(predictions, testing$int_rate)
)

```

## Q7. Answer the following questions {-}
a. Which model performs best out of sample, OLS regression or LASSO? Why?
b. What value of lambda offers best performance? Is this sensitive to the random seed? Why?
c. How many coefficients are zero and how many are non-zero in the LASSO model of best fit? Is number of zero (or non-zero) coefficients sensitive on the random seed? Why?
d. Why is it important to standardize continuous variables before running LASSO? 

>Answer here: 

a. LASSO performs better than OLS regression. This is because using LASSO, it shrinks the non-zero coefficients towards zero by penalizing the coefficients that introduce high error with low explanatory power. LASSO also reduces multicolinearity issue and overfitting issue of the model. Therefore, LASSO performs better than OLS regression. 

b. When the seed is 1234, the value of lambda offering best performance is 0.0003103103. This is sensitive to the random seed for example when the seed is 1, lambda will change to 9.009009e-05, which has around 70% of difference from the best lambda when the seed is 1234. This is because the randomness of train-test split will result in different models, i.e., coefficients and residual error. Therefore, the penalty term lambda will have different degree of penalization on coefficients.  

c. When the seed is 1234, 31 coefficients are zero, while 27 coefficients are non-zero in the LASSO model of best fit. This is sensitive to the random seed for example when the seed is 1, only 16 coefficients are zero, while 42 coefficients are non-zero in the LASSO model of best fit. As explained above in Q7-b, this is because the randomness of train-test split will result in different models, i.e., coefficients and residual error. Therefore, LASSO will penalize different coefficients for the model to avoid overfitting. 

d. For example, when loan_amnt coefficient is 1.528e-07, meaning that for each million increase of loan amount, the interest rate would increase by 15%. Comparably, the dti coefficient is 3.793e-03, meaning that for each four thousand increase of dti, the interest rate would increase by 15%. The penalization term in LASSO equation can be much smaller for loan_amnt with smaller coefficients compared to dti, given that lambda is constant. Therefore, LASSO would penalize dti more than loan_amnt because of their difference in scales. Using regularization can standardize the different units of different continuous variables, which solves the difference-in-scale problem, thus making the coefficients with different units comparable among each other. 

# Using Time Information

Let's try to further improve the model's predictive performance. So far we have not used any time series information. Effectively, all things being equal, our prediction for the interest rate of a loan given in 2009 would be the same as that of a loan given in 2011. Is this a good assumption?
 
First, investigate graphically whether there are any time trends in the interest rates. (Note that the variable "issue_d" only has information on the month the loan was awarded but not the exact date.) Can you use this information to further improve the forecasting accuracy of your model? Try controlling for time in a linear fashion (i.e., a linear time trend) and controlling for time as quarter-year dummies (this is a method to capture non-linear effects of time -- we assume that the impact of time doesn't change within a quarter but it can chance from quarter to quarter). Finally, check if time affect loans of different grades differently.

```{r, time_trends,warning=FALSE, message=FALSE}

#linear time trend (add code below)
lc_clean$issue_d = as.Date(lc_clean$issue_d)

#add a column which indicates year-quarter
lc_clean <- lc_clean %>% 
  mutate(qtr= as.yearqtr(issue_d))

#calculate the mean interest rate for each quarter
lc_clean_time <- lc_clean %>% 
  select(int_rate, qtr) %>%
    #since there is only 1 observation in 2007-Q2 which would not be representative for the entire quarter, we decide to remove it
  filter(qtr != "2007 Q2") %>% 
  group_by(qtr) %>% 
  summarise(avg_int_rate = mean(int_rate))

#plot the time series average interest rate based on quarters
time_trend <- lc_clean_time %>% 
  ggplot(aes(x = qtr,y = avg_int_rate))+
  geom_line()+
  scale_y_continuous(labels = scales::percent)+
  labs(title = "How interest rates vary with time",
       subtitle = "with time expressed in year-quarter",
      x = "Year-Quarter",
       y = "Average Interest Rate")+
  theme_bw()

#visualize
time_trend

```
Based on the graph above, we can see that over the year 2007 to 2009, interest rate keeps increasing. During 2009 to 2011, the interest rate remains relatively stable with only a small trough in 2010-Q3. The interest rate picked up afterwards. We can see that year-quarter does have an impact on the difference in interest rates and therefore it is necessary to introduce it in the form of dummy variable to the model. 


```{r,time_trends_by_grade,warning=FALSE, message=FALSE}
#linear time trend by grade (add code below)
lc_clean_grade <- lc_clean %>% 
  select(int_rate, grade, qtr) %>% 
  #since there is only 1 observation in 2007-Q2 which would not be representative for the entire quarter, we decide to remove it
  filter(qtr != "2007 Q2") %>% 
  group_by(qtr,grade) %>% 
  summarise(avg_int_rate = mean(int_rate))

#plot the time series average interest rate based on quarters
time_trend_by_grade <- lc_clean_grade %>% 
  ggplot(aes(x = qtr,y = avg_int_rate, group = grade))+
  geom_line(aes(color=grade))+
  facet_wrap(~grade)+
  labs(title = "How interest rates vary with time for different credit groups",
       subtitle = "with time expressed in year-quarter",
      x = "Year-Quarter",
       y = "Average Interest Rate")+
  theme_bw()

#visualize
time_trend_by_grade
```
Based on the graph above, grade C to G has an increase of interest rate over time, while grade A and B remains relatively stable. We can also see that before 2011, interest rate grows steadily in grade C to G while after 2011 it spiked. 

```{r,time_model_0,warning=FALSE, message=FALSE}
#Train models using OLS regression and k-fold cross-validation
#base model without time trend variable
time0<-train(
  #explanatory variables
  int_rate ~ loan_amnt + term + loan_amnt*grade + log(annual_inc),
  lc_clean,
  method = "lm",
  trControl = control)

summary(time0)
```

```{r,time_model_1,warning=FALSE, message=FALSE}

#Train models using OLS regression and k-fold cross-validation
#The first model has some explanatory variables and a linear time trend

#convert issue_d date variable from discrete to continuous variable
temp <- lc_clean
temp$issue_d=as.POSIXct(temp$issue_d)
min=min(temp$issue_d)
temp <- temp %>% 
  mutate(seq_month = as.numeric(difftime(temp$issue_d,min,units='days'))/30)

#fit the model
time1<-train(
  #explanatory variables
  int_rate ~ loan_amnt + term + loan_amnt*grade + log(annual_inc) +
    #time variable of sequencial number of the month
    seq_month,
  temp,
  method = "lm",
  trControl = control)

summary(time1)
```

We can see that based on the result above, the time variable `seq_month` has significant explanatory power since the p-value is <2e-16, which is less than 5%. Compared to model0, the rse decreased from 0.01052 to 0.01031, i.e., 2%; the R^2 increased from 0.9203 to 0.9234, i.e., 0.34%. We can conclude by including time variable into our model, the performance is better. 

```{r,time_model_2,warning=FALSE, message=FALSE}

#The second model has a different linear time trend for each grade class
time2<-train(
    int_rate ~ loan_amnt + term + loan_amnt*grade + log(annual_inc) + 
      grade +grade:seq_month, #fill your variables here 
    temp,
   method = "lm",
    trControl = control
   )

summary(time2)

```
We can see that by adding different time trend to each grade class, it generates 7 more interaction variable coefficients than model2. For instance, `gradeA:seq_month` has an coefficient of -4.272e-04, which corresponds to the negative correlation between time and interest rate for grade A in the previously plotted graph in "How interest rates vary with time for different credit groups". This means for grade A, by each 10 months increase, the interest rate will drop 0.4272%, while for grade B, the interest rate will increase by 0.1075%. 
For all the 7 additional interaction term, the p-values have significant explanatory power since they are less than 5%. Compared to model1, the rse decreased from 0.01031 to 0.009054, i.e., 12.18%; the R^2 increased from 0.9234 to 0.941, i.e., 1.9%. We can conclude by including interaction variable `grade:seq_month` variable into our model, the performance is better.

```{r,time_model_3,warning=FALSE, message=FALSE}
#Change the time trend to a quarter dummy variables.
#zoo::as.yearqrt() creates quarter dummies 
lc_clean_quarter<-lc_clean %>%
  mutate(yq = as.factor(as.yearqtr(lc_clean$issue_d, format = "%Y-%m-%d")))



time3<-train(
    int_rate ~ loan_amnt + term + loan_amnt*grade + log(annual_inc) + grade +
    yq,#fill your variables here 
    lc_clean_quarter,
     method = "lm",
    trControl = control
   )
  
summary(time3)
```
We can see that based on the result above, the dummy time variable `yq` has significant explanatory power in most quarters since the p-values are <5%. For instance, `yq2007 Q3` has coefficient of 1.926e-02, which means the interest rate is 0.01926% more compared to `yq2007 Q2`. For instance, `yq2007 Q4` has coefficient of 1.480e-02, which means the interest rate is 0.0148% more compared to `yq2007 Q2`. So on and so forth. 
Compared to model1, the rse decreased from 0.01031 to 0.00907, i.e., 12%; the R^2 increased from 0.9234 to 0.9408, i.e., 1.88%. We can conclude by including quarter variable `yq` into our model, the performance is better than the one including the month variable `seq_month`. 


```{r,time_model_4,warning=FALSE, message=FALSE}
#We specify one quarter dummy variable for each grade. This is going to be a large model as there are 19 quarters x 7 grades = 133 quarter-grade dummies.
time4<-train(
    int_rate ~  loan_amnt + term + loan_amnt*grade + log(annual_inc) + grade +
    grade:yq, #fill your variables here 
    lc_clean_quarter,
     method = "lm",
    trControl = control
   )

summary(time4)
```
We can see that based on the result above, the interaction variable `grade:yq` has significant explanatory power in most cases since the p-values are <5%. For instance, `gradeB:yq2007 Q3` has a coefficient of `-2.211e-02`, which means the interest rate is 2.211% more compared to grade B in 2007 Q2.  
Compared to model3, the rse decreased from 0.00907 to 0.007569, i.e., 16.5%; the R^2 increased from 0.9408 to 0.9587, i.e., 1.9%. We can conclude by including interaction variable `grade:yq` into our model, the performance is better than the one including the month variable `yq`. 


## Q8 Based on your analysis above, is there any evidence to suggest that interest rates change over time? Does including time trends / quarter-year dummies improve predictions? Any improvement in prediction performance? {-}

>Answer here:

```{r,compare_time_models,warning=FALSE, message=FALSE}

#organising our results
data.frame(
  #base model variables
  time0$results$RMSE, 
  time0$results$Rsquared,
  
  #base model variables + month variable
  time1$results$RMSE,
  time1$results$Rsquared,
  
  #base model variables + month variable*grade
  time2$results$RMSE,
  time2$results$Rsquared,
  
  #base model variables + quarter variable
  time3$results$RMSE,
  time3$results$Rsquared,
  
  #base model variables + quarter variable*grade
  time4$results$RMSE,
  time4$results$Rsquared)

```

Compared to model0, model1's rse decreased from 0.01052 to 0.01031, i.e., 2%; the R^2 increased from 0.9203 to 0.9234, i.e., 0.34%. We can conclude by including time variable into our model, the performance is better. 

Compared to model1, model2's rse decreased from 0.01031 to 0.009054, i.e., 12.18%; the R^2 increased from 0.9234 to 0.941, i.e., 1.9%. We can conclude by including interaction variable `grade:seq_month` variable into our model, the performance is better.

Compared to model1, model3's rse decreased from 0.01031 to 0.00907, i.e., 12%; the R^2 increased from 0.9234 to 0.9408, i.e., 1.88%. We can conclude by including quarter variable `yq` into our model, the performance is better than the model including the month variable `seq_month`. 

Compared to model3, model4's rse decreased from 0.00907 to 0.007569, i.e., 16.5%; the R^2 increased from 0.9408 to 0.9587, i.e., 1.9%. We can conclude by including interaction variable `grade:yq` into our model, the performance is better than the model including the month variable `yq`. 

In summary, we can see that by separating each grade and bring in time trend as variables, we will arrive at better prediction. And quarter variable has more explanatory power than month variable.  

# Using Bond Yields 
One concern with using time trends for forecasting is that in order to make predictions for future loans we will need to project trends to the future. This is an extrapolation that may not be reasonable, especially if macroeconomic conditions in the future change. Furthermore, if we are using quarter-year dummies, it is not even possible to estimate the coefficient of these dummy variables for future quarters.

Instead, perhaps it's better to find the reasons as to why different periods are different from one another. The csv file "MonthBondYields.csv" contains information on the yield of US Treasuries on the first day of each month. Can you use it to see if you can improve your predictions without using time dummies? 


```{r, bond yields,warning=FALSE, message=FALSE}
#load the data to memory as a dataframe
bond_prices<-readr::read_csv("MonthBondYields .csv")

#make the date of the bond file comparable to the lending club dataset
#for some regional date/number (locale) settings this may not work. If it does try running the following line of code in the Console
#Sys.setlocale("LC_TIME","English")
bond_prices <- bond_prices %>%
  mutate(Date2=as.Date(paste("01",Date,sep="-"),"%d-%b-%y")) %>%
  select(-starts_with("X"))

#let's see what happened to bond yields over time. Lower bond yields mean the cost of borrowing has gone down.
bond_prices %>%
  ggplot(aes(x=Date2, y=Price))+geom_line() + 
  labs(x="Month of the year",
       y="Bond Price",
       title="Bond price change over time")
```
We can see that as over the course of 2007 to 2012, the bond price is quite volatile and follows a general trend of decreasing with no cyclical pattern. The bond price dipped right before 2009 and bounced back at the start of 2009. It also dipped over the first half year of 2010 before it bounced back. 

```{r, bond yields_1,warning=FALSE, message=FALSE}
#join the data using a left join
lc_with_bonds<-lc_clean %>%
  left_join(bond_prices, by = c("issue_d" = "Date2")) %>%
  arrange(issue_d) %>%
  filter(!is.na(Price)) #drop any observations where there re no bond prices available

# investigate graphically if there is a relationship 
lc_with_bonds%>%
  ggplot(aes(x=int_rate, y=Price))+
  geom_point(size=0.1, alpha=0.5)+
  geom_smooth(method="lm")+
    labs(x="Interest rate",
       y="Bond price",
       title="How interest rate changes based on changes in bond price")
```
Based on the graph, we can see that the observations are quite scattered but follows a slightly negative correlation between interest rate and bond price. 

```{r, bond yields_3,warning=FALSE, message=FALSE}
#plot the relationship separately within each grade class
lc_with_bonds%>%
  ggplot(aes(x=int_rate, y=Price, color=grade))+
  geom_point(size=0.1, alpha=0.5)+
  geom_smooth(method="lm")+
  labs(x="Interest rate",
       y="Bond price",
       title="How interest rate changes based on changes in bond price for each lending grade",
       guide_legend="Grade")

```
However, when separated into lending grades, the negative correlation between interest rate and bond price becomes stronger within each grade. Therefore, we add the interaction term `grade:Price` in the following model. 

```{r, bond yields_4,warning=FALSE, message=FALSE}
#let's fit the model by incorporating the bond price information with grade
model_bond<-train(
    int_rate ~ loan_amnt + term + loan_amnt*grade + log(annual_inc) + grade +
    grade:Price , #fill your variables here 
    lc_with_bonds,
   method = "lm",
    trControl = control
   )
summary(model_bond)
```

## Q9. Do bond yields have any explanatory power? Do not forget to interpret coeficicents.

>
* We can see from above that for each interaction term `grade:Price`, the coefficient is significant in that p-value = 2e-16, which is less than 5%. And the coefficient of `gradeA:Price` with 1.281e-03 means that for example if bond price increased from 1% yield to 11% yield, there will be 1.281% increase in interest rate for the grade A lenders. Likewise, the coefficient of `gradeB:Price` with -4.484e-03 means that if bond price increased from 1% yield to 11% yield, there will be 4.484% increase in interest rate for grade B lenders. 
* Compared to model0, this model's rse decreased from 0.01052 to 0.009584, i.e., 8.9%; the R^2 increased from 0.9203 to 0.9339, i.e., 1.48%. We can conclude that by including bond price into our model, the performance is better. 
* Although time trend is a significant variable in the previous model, it is not predictive for future trends of interest rates as discussed above. Thus, instead of using time as a variable, bond price has more explanatory power for the future in this sense. 


## Q10. Choose a model and describe your methodology {-}
Feel free to investigate more models with different features using the methodologies covered so far. Present the model you believe predicts interest rates the best. Describe how good it is (including the length of the 95% Confidence Interval of predictions that use this model) and what features it uses. What methodology did you use to choose it? (Do not use time trends or quarter-year dummies in your model as the first cannot be extrapolated into the future reliably and the second cannot be even estimated for future quarters.)

>
The best model is described in the chunk below with the outlined variable. When choosing the best model out of all the ones constructed in previous questions, we compared the RMSE of each model and selected the one with the lowest RMSE which does not include time-trend variables. Secondly, the significance of each variables are considered when choosing the best combination of variables to avoid overfitting. For example, by including "dti", we can actually improve the RMSE further by 0.000001. However, this complicates the model and this complexity further adds to the minor explanarotory power of "dit" in this model.

```{r,bestmodel,message=FALSE,warning=FALSE}
bestmodel<-train(
    int_rate ~ loan_amnt + term + loan_amnt:grade + log(annual_inc) + grade + 
    Price + grade:Price , 
    lc_with_bonds,
   method = "lm",
    trControl = control
   )
summary(bestmodel)

```



## Q11. Use other publicly available datasets to further improve performance (e.g., quarterly data on US inflation or [CPI](https://fred.stlouisfed.org/series/CPALTT01USQ657N)).


# Explain why you think the additional data will make a difference and check if it does.{-}

In previous questions, it was found that time variables (i.e. issue_d, yq, qtr...), which is a significant variable in the model, does help improve the RMSE of the model but it has low explanatory power. By including consumer price index, we can include information on inflation over time, thus replacing the time variables. In fractional reserve banking, CPI is inversely related to interest rate, thus it is expected that there is going to be a negative gradient for CPI in the model.

```{r,cpi,message=FALSE,warning=FALSE}
# Import the consumer price index data 
cpi <- read_csv("ConsumerPriceIndex.csv")

cpi_clean <- cpi %>%
  mutate(date = ymd(DATE)) %>% #convert to y-m-d format
  mutate(cpi_delta = 
           (CPALTT01USQ657N - lag(CPALTT01USQ657N))/lag(CPALTT01USQ657N), #calculate the percentage change in cpi
         yq = as.yearqtr(cpi$DATE, format = "%Y-%m-%d")) %>%  #convert the DATE variable into quarters
  filter(date >= "2007-07-01" & date <= "2011-12-01") %>% #filter for the desired date that match with our previous tables
  select(date,yq,cpi_delta,CPALTT01USQ657N) %>% #select the desired coloumns
  rename(cpi = CPALTT01USQ657N) 

glimpse(cpi_clean)
```

```{r,int_cpi,message=FALSE,warning=FALSE}
int_cpi <- lc_with_bonds %>% 
  group_by(qtr) %>% 
  summarise(mean_int_rate = mean(int_rate)) %>% #calculate mean quarterly interest rate
  mutate(int_delta = (mean_int_rate - lag(mean_int_rate))/lag(mean_int_rate)) %>% #calculate change in interest rate
  left_join(cpi_clean, by = c("qtr" = "yq"))

int_cpi[is.na(int_cpi)] <- 0 #The first int_delta will be NA

glimpse(int_cpi)
```


```{r,lc_with_cpi,warning=FALSE,message=FALSE}
# Join the dataframes
lc_with_cpi <- lc_with_bonds %>% 
  arrange(issue_d) %>% #arrange by issue date so that we can use fill to autofill values for months with NA
  left_join(cpi_clean, by = c("issue_d" = "date")) %>% 
  fill(cpi_delta) %>% 
  rename(Grade = grade)

  
glimpse(lc_with_cpi)
```


```{r,cpi_plot,message=FALSE,warning=FALSE}

# Visualise the relationship
int_cpi %>% 
  ggplot(aes(x = cpi_delta, y = mean_int_rate)) +
  geom_point(size = 0.8) +
  geom_smooth(se = FALSE)+ #line of best fit
  scale_x_continuous(labels = scales::percent)+ #convert x axis to percentage scale
  scale_y_continuous(labels = scales::percent)+ #convert y axis to percentage scale
  labs(title = "How interest rates vary with change in consumer price index", 
       x = "Quarterly change in consumer price index (%)",
       y = "Mean quarterly Interest Rate")+
  theme_bw()

```

As we can see in the above plot, when there is a quarterly cpi change between -200% to +200%, the quarterly interest rate will increase more slowly as the quarterly cpi change increases. In other words, the faster cpi increases, the more slowly interest rate increases. Although the quarterly interest rate is calculated by including time information, which is not a good predictor for interest rate, we can still say there is a correlation between cpi_delta and interest rate as cpi_delta also includes time information. These varaibles run parallel in time, and we are using this plot to show that they correlate to each other during this time period, rather than saying that either of them is affected by time.

```{r,newmodel,warning=FALSE,message=FALSE}
newmodel<-train(
    int_rate ~ loan_amnt + term + loan_amnt:Grade + log(annual_inc) + Grade + 
    Price + Grade:Price + #variables from best model
    cpi_delta + Grade:cpi_delta + term:cpi_delta, #variables derived from cpi
    data = lc_with_cpi,
    method = "lm",
    trControl = control
   )
summary(newmodel)
```

* In the best model we have derived in Q10, which is from the available variables in the lc dataframe, the RMSE is 0.9584 while in the model built here with CPI, the RMSE is 0.8965. All the CPI (i.e. cpi_delta) related variables are highly significant in the model.
